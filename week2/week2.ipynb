{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning 2025 — Homework 2\n",
        "\n",
        "## Linear Regression: From Scratch and with PyTorch\n",
        "\n",
        "\n",
        "Goal: Understand how the math behind linear regression translates into code — first by hand with Numpy, then using PyTorch."
      ],
      "metadata": {
        "id": "kQAQo64r7XnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "In this assignment, you will:\n",
        "1. Implement a linear regression model from scratch using NumPy.\n",
        "2. Implement the same model using PyTorch, and compare both approaches.\n",
        "3. Apply your model to a real-world dataset (California housing data).\n",
        "\n",
        "This will help you connect the mathematical concepts from the lectures with their practical implementation.\n"
      ],
      "metadata": {
        "id": "otW9K09c7ujz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1 — Linear Regression from Scratch\n",
        "You will implement all the components manually:\n",
        "\n",
        "- Linear model: $ ( \\hat{y} = Xw + b ) $\n",
        "- Mean squared error loss\n",
        "- Gradient descent\n",
        "\n",
        "**Hint:** Revisit your notes from matrix calculus — the gradient of MSE with respect to w and b can be derived using the chain rule.\n",
        "\n",
        "**Concept Reminder:**\n",
        "- The model parameters `w` and `b` define a line that tries to best fit the data.\n",
        "- The *loss* measures how far predictions are from true values.\n",
        "- The *gradient* tells us how to move `w` and `b` to reduce that loss."
      ],
      "metadata": {
        "id": "wzT8Qjpt8dBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1. Generate synthetic data\n"
      ],
      "metadata": {
        "id": "H6ZHlXswV1zE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(22)\n",
        "\n",
        "# Generate 100 samples of x and corresponding y\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 3 * X + 7 + np.random.randn(100, 1) * 0.5\n",
        "\n",
        "plt.scatter(X, y, color='blue', label='Data')\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Synthetic Data: y = 3x + 7 + noise\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K1jLNgcH9z5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2. Implement the linear model and loss\n",
        "Define two Python functions:\n",
        "- `predict(X, w, b)` → compute predictions \\(Xw + b\\)\n",
        "- `mse_loss(y_pred, y_true)` → compute mean squared error\n",
        "\n",
        " **Concept Reminder:**  \n",
        "- `predict(X, w, b)` performs a linear transformation of the input data.  \n",
        "- The loss function measures how well our model’s predictions match the target values.  \n",
        "- Minimizing the loss helps the model learn the best `w` and `b`."
      ],
      "metadata": {
        "id": "IxvPjaBaWSaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement the predict and loss functions\n",
        "\n",
        "def predict(X, w, b):\n",
        "    # return...\n",
        "    pass\n",
        "\n",
        "def mse_loss(y_pred, y_true):\n",
        "    # ...\n",
        "    pass"
      ],
      "metadata": {
        "id": "kaUX5g9W-C14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3. Implement gradient descent\n",
        "Compute gradients and update weights iteratively.\n",
        "\n",
        " **Hint:** We start with a small random `w` and `b = 0`. Use a learning rate around `0.1` and about `100` epochs.\n",
        "\n",
        " **Concept Reminder:**\n",
        "- The gradient $ \\frac{\\partial L}{\\partial w} $ indicates how much the loss changes when we slightly change `w`.  \n",
        "- Gradient descent repeatedly moves `w` and `b` in the direction that reduces the loss.  \n",
        "- The learning rate `η` controls how large each step is.\n",
        "\n",
        "The gradients are:\n",
        "\n",
        "$ \\frac{\\partial L}{\\partial w} = \\frac{2}{n} X^T (Xw + b - y) $\n",
        "\n",
        "$ \\frac{\\partial L}{\\partial b} = \\frac{2}{n} \\sum_i (Xw + b - y)_i $\n",
        "\n",
        "(We saw them in our tutorial session as well)"
      ],
      "metadata": {
        "id": "-BlvBn89XPob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: initialize parameters and run training loop\n",
        "w = np.random.randn(1, 1)\n",
        "b = np.zeros((1, 1))\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.1\n",
        "epochs = 100\n",
        "losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # 1. Predict\n",
        "    y_pred = predict(X, w, b)\n",
        "\n",
        "    # 2. Compute loss\n",
        "    loss = mse_loss(y_pred, y)\n",
        "\n",
        "    # 3. Compute gradients (dw, db)\n",
        "    # TODO: derive and implement gradient update rules here\n",
        "\n",
        "    # 4. Update parameters\n",
        "    # TODO: update w and b\n",
        "\n",
        "    losses.append(loss)\n",
        "\n",
        "print(f\"Learned parameters: w = {w.item():.3f}, b = {b.item():.3f}\")\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss (MSE)\")\n",
        "plt.title(\"Training Loss over Epochs\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UAcLzgcZ-G-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4. Visualize the result\n",
        "Plot the fitted line on top of your data.\n",
        "\n",
        " **Concept Reminder:**\n",
        "- After training, your line should closely match the true relation.  \n",
        "- If the learning rate is too high, the line might overshoot and fail to converge.\n"
      ],
      "metadata": {
        "id": "Oa5DSfYyYavv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X, y, label=\"Data\", alpha=0.6)\n",
        "plt.plot(X, predict(X, w, b), color='red', label=\"Fitted Line\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.title(\"Fitted Linear Regression Line\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OruOO0QC-K9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2 — Linear Regression with PyTorch\n",
        "Now we’ll repeat the same experiment using PyTorch, which will handle the gradient computation for you.\n",
        "\n",
        "**Concept Reminder:**\n",
        "- PyTorch uses automatic differentiation (`autograd`) to compute gradients.\n",
        "- Instead of manually implementing the gradient formulas, we define the model and let PyTorch handle the math.\n",
        "- The optimization process (SGD) follows the same logic as before — but under the hood.\n"
      ],
      "metadata": {
        "id": "AhFM_VYg_-jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "# Convert your NumPy arrays to torch tensors\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "# TODO: Define model, loss, and optimizer\n",
        "# model = nn.Linear(...)\n",
        "# criterion = nn.MSELoss()\n",
        "# optimizer = optim.SGD(...)"
      ],
      "metadata": {
        "id": "rysh0Kb1AGyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2. Training Loop\n",
        "Implement the typical PyTorch training cycle:\n",
        "1. Forward pass → compute predictions\n",
        "2. Compute loss → how wrong is the model?\n",
        "3. Zero gradients → reset previous gradients\n",
        "4. Backward pass → compute new gradients\n",
        "5. Optimizer step → update parameters\n",
        "\n",
        " **Concept Reminder:**  \n",
        "This mirrors the NumPy loop, but now gradients are handled automatically by PyTorch.\n"
      ],
      "metadata": {
        "id": "3eblaMWbZAE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # TODO: forward, compute loss, backward, step\n",
        "    pass\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"PyTorch Training Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HMyOwfmtAOQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3. Compare both models\n",
        "Compare learned parameters between your NumPy and PyTorch implementations. The parameters should be very close if both models were trained correctly.\n"
      ],
      "metadata": {
        "id": "xhKtKjjfZuNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: extract and print learned parameters"
      ],
      "metadata": {
        "id": "31Z0iEiTAWAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3 — Real Dataset: California Housing\n",
        "Apply your PyTorch linear regression model to a real dataset.\n",
        "\n",
        "We'll use the California housing dataset (you can load it directly using the sklearn python package) to predict house prices from one feature.\n",
        "\n",
        "**Concept Reminder:**\n",
        "- Real datasets rarely follow perfect linear relationships.  \n",
        "- Still, linear regression provides a good first baseline and a visual introduction to model fitting.\n"
      ],
      "metadata": {
        "id": "5XM396S4An4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing(as_frame=True)\n",
        "print(data.feature_names)\n",
        "\n",
        "# Choose one feature and the target\n",
        "X_real = data.data[['AveRooms']].values\n",
        "y_real = data.target.values.reshape(-1, 1)\n",
        "\n",
        "plt.scatter(X_real, y_real, s=5, alpha=0.5)\n",
        "plt.xlabel('Average number of rooms')\n",
        "plt.ylabel('Median house value ($100,000s)')\n",
        "plt.title('California Housing: AveRooms vs. Price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H-PyeK8RAzTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2. Train a linear model on the real data\n",
        " **Hint:** Start from your PyTorch code above. Adjust the learning rate — this dataset has a different scale.\n",
        "\n",
        " **Concept Reminder:**\n",
        "- Larger datasets might require smaller learning rates or more epochs.  \n",
        "- The goal is to minimize the loss — even if the line doesn’t perfectly fit the points.\n"
      ],
      "metadata": {
        "id": "o1WHckcSaId4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: reuse PyTorch model and training loop for real dataset. Plot your training loss as we did before too."
      ],
      "metadata": {
        "id": "UnTB3obCBX7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Visualize your results\n",
        "Plot predictions vs. actual data points. A single linear feature often cannot fully capture complex dependencies — but you can observe the overall trend.\n"
      ],
      "metadata": {
        "id": "4BhwSZjJafnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: visualize predictions vs. ground truth"
      ],
      "metadata": {
        "id": "SwmEIRp_BbS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional (will not be graded)\n",
        "You can also try adding more input features to see how the model behaves."
      ],
      "metadata": {
        "id": "TGvm9vTVBrGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "SV6DoyLaa3JV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework submission\n",
        "Upload your answer file to Moodle, preferably with all the code cells executed."
      ],
      "metadata": {
        "id": "YXMMrfxxCFs1"
      }
    }
  ]
}