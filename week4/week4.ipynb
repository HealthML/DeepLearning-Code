{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning 2025 — Homework Week 4\n",
        "## Logistic Regression (sigmoid) & Softmax Regression\n",
        "**Focus:** Implement classification from scratch.  \n",
        "**Compare:** Stochastic Gradient Descent (SGD) vs Newton’s Method (for logistic).  \n",
        "**Deliverables:** Filled notebook with code\n",
        "\n",
        "---\n",
        "\n",
        "### Learning goals\n",
        "- Implement sigmoid and log-loss with numerically stable code.\n",
        "- Derive/implement gradients and Hessians for logistic regression.\n",
        "- Compare **SGD** vs **Newton** (speed, iterations, wall time, robustness).\n",
        "- Implement **softmax regression** for multiclass classification.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LD5KlSNN8b2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup & Utilities"
      ],
      "metadata": {
        "id": "wpYsRJvAtU43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "np.random.seed(42)\n",
        "\n",
        "# Small helpers for visualizing decision boundaries\n",
        "\n",
        "def add_bias(X):\n",
        "    return np.hstack([X, np.ones((X.shape[0], 1))])\n",
        "\n",
        "def plot_boundary(w, X, y, title):\n",
        "    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
        "    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
        "    Xg = np.c_[xx.ravel(), yy.ravel()]\n",
        "    Xgb = add_bias(Xg)\n",
        "    z = 1/(1+np.exp(-(Xgb @ w)))  # for visualization only\n",
        "    Z = (z > 0.5).astype(int).reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.2, cmap=\"bwr\")\n",
        "    plt.scatter(X[:,0], X[:,1], c=y.ravel(), cmap=\"bwr\", s=12, edgecolor='k')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# Toy datasets\n",
        "\n",
        "def make_toy_binary(n=300, centers=((0,0), (2,2)), std=0.9):\n",
        "    n1 = n//2; n2 = n - n1\n",
        "    X1 = np.random.randn(n1,2)*std + np.array(centers[0])\n",
        "    X2 = np.random.randn(n2,2)*std + np.array(centers[1])\n",
        "    X = np.vstack([X1, X2])\n",
        "    y = np.vstack([np.zeros((n1,1)), np.ones((n2,1))])\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def make_toy_multiclass(n=450, centers=((0,0),(2.2,0),(1.1,2.0)), std=0.7):\n",
        "    per = n//3\n",
        "    Xs, ys = [], []\n",
        "    for c, mu in enumerate(centers):\n",
        "        Xc = np.random.randn(per,2)*std + np.array(mu)\n",
        "        yc = np.full((per,1), c)\n",
        "        Xs.append(Xc); ys.append(yc)\n",
        "    X = np.vstack(Xs); y = np.vstack(ys).astype(int)\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "bQ0HsqatxiVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1 — Binary Logistic Regression: SGD vs Newton (from scratch)\n",
        "We'll use a 2D dataset so you can visualize the boundary. You must fill core math pieces.\n"
      ],
      "metadata": {
        "id": "cNH6ssWKxzl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "X, y = make_toy_binary(n=300)\n",
        "Xb = add_bias(X)\n",
        "plt.scatter(X[:,0], X[:,1], c=y.ravel(), cmap=\"bwr\", s=12)\n",
        "plt.title(\"Toy binary dataset\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "peizqZN38jtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Sigmoid and Stable Log-Loss (Given)\n",
        "We provide numerically stable sigmoid and negative log-likelihood loss (with L2 on weights, not on bias).\n"
      ],
      "metadata": {
        "id": "E1Zvpnyo8cAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GIVEN\n",
        "\n",
        "def sigmoid(z):\n",
        "    # stable sigmoid\n",
        "    out = np.empty_like(z)\n",
        "    pos = z >= 0\n",
        "    neg = ~pos\n",
        "    out[pos] = 1.0 / (1.0 + np.exp(-z[pos]))\n",
        "    ez = np.exp(z[neg])\n",
        "    out[neg] = ez / (1.0 + ez)\n",
        "    return out\n",
        "\n",
        "\n",
        "def logloss(w, Xb, y, l2=0.0):\n",
        "    z = Xb @ w\n",
        "    p = sigmoid(z)\n",
        "    eps = 1e-12\n",
        "    p = np.clip(p, eps, 1-eps)\n",
        "    n = Xb.shape[0]\n",
        "    ll = -np.mean(y*np.log(p) + (1-y)*np.log(1-p)) + 0.5*l2*np.sum(w[:-1]**2)/n\n",
        "    return ll"
      ],
      "metadata": {
        "id": "mBGEMsz28u2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Gradients and Hessian (TODO)\n",
        "Use matrix calculus to implement:\n",
        "- Gradient:  $\\nabla L(w) = \\frac{1}{n} X_b^T (p - y) + \\lambda \\tilde{w} $\n",
        "- Hessian:  $ H = \\frac{1}{n} X_b^T S X_b + \\lambda \\tilde{I}$, where $ S=diag(p(1-p)) $\n",
        "\n",
        "Hints:\n",
        "- Do **not** regularize the bias (last row/entry).\n",
        "- Keep shapes: `w` is (d,1), `Xb` is (n,d), `y` is (n,1).\n"
      ],
      "metadata": {
        "id": "NGyhgCyl-Zyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement grad() and hessian()\n",
        "\n",
        "def grad(w, Xb, y, l2=0.0):\n",
        "    # TODO\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def hessian(w, Xb, y, l2=0.0):\n",
        "    # TODO (you may build S as a vector and use broadcasting; diag is fine for clarity)\n",
        "    raise NotImplementedError"
      ],
      "metadata": {
        "id": "VlqxKDD19GbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Optimizers\n",
        "Implement two training routines:\n",
        "- **Mini-batch SGD** (fill the update line using grad on each batch)\n",
        "- **Newton's method** (full batch each step; we give damping and loop skeleton)\n"
      ],
      "metadata": {
        "id": "YuyOVqLw-6bu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement SGD optimizer\n",
        "\n",
        "def fit_logistic_sgd(Xb, y, l2=1e-3, lr=0.2, epochs=50, batch_size=32, verbose=False):\n",
        "    n, d = Xb.shape\n",
        "    w = np.zeros((d,1))\n",
        "    hist = []\n",
        "    idx = np.arange(n)\n",
        "    for ep in range(epochs):\n",
        "        np.random.shuffle(idx)\n",
        "        for start in range(0, n, batch_size):\n",
        "            batch = idx[start:start+batch_size]\n",
        "            Xb_b, y_b = Xb[batch], y[batch]\n",
        "            # TODO: compute gradient on batch and update w\n",
        "            # g = ...\n",
        "            # w -= lr * g\n",
        "            pass\n",
        "        hist.append(logloss(w, Xb, y, l2=l2))\n",
        "    return w, hist"
      ],
      "metadata": {
        "id": "yam2yi-7_SlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement Newton optimizer (given damping + skeleton)\n",
        "\n",
        "def fit_logistic_newton(Xb, y, l2=1e-3, max_iter=10, damping=1e-4, verbose=False):\n",
        "    n, d = Xb.shape\n",
        "    w = np.zeros((d,1))\n",
        "    hist = []\n",
        "    for it in range(max_iter):\n",
        "        # TODO: compute full-batch gradient and Hessian\n",
        "        # g = grad(...)\n",
        "        # H = hessian(...)\n",
        "        # Solve (H + damping*I) * step = g\n",
        "        # step = np.linalg.solve(H_damped, g)\n",
        "        # w = w - step\n",
        "        # L = logloss(w, Xb, y, l2=l2)\n",
        "        # hist.append(L)\n",
        "        pass\n",
        "    return w, hist"
      ],
      "metadata": {
        "id": "6vfBoOJc_hFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Train & Compare\n",
        "Plot loss curves for both methods and visualize the learned decision boundaries.\n"
      ],
      "metadata": {
        "id": "_yGpIqRy_O_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After you implement the functions above, uncomment:\n",
        "# w_sgd, hist_sgd = fit_logistic_sgd(Xb, y, l2=1e-3, lr=0.2, epochs=50, batch_size=32)\n",
        "# w_newton, hist_newton = fit_logistic_newton(Xb, y, l2=1e-3, max_iter=10, damping=1e-4)\n",
        "#\n",
        "# plt.plot(hist_sgd, label=\"SGD loss\")\n",
        "# plt.plot(hist_newton, label=\"Newton loss\")\n",
        "# plt.xlabel(\"Epoch / Iter\")\n",
        "# plt.ylabel(\"Loss\")\n",
        "# plt.legend(); plt.title(\"Convergence comparison\")\n",
        "# plt.show()\n",
        "#\n",
        "# plot_boundary(w_sgd, X, y, \"Decision boundary — SGD\")\n",
        "# plot_boundary(w_newton, X, y, \"Decision boundary — Newton\")\n"
      ],
      "metadata": {
        "id": "6KCHvcXPbK6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Short reflections (answer in Markdown)\n",
        "1) Which converges in fewer iterations?  \n",
        "2) Which is faster in wall-clock time on this dataset?  \n",
        "3) How do batch size and learning rate affect SGD?  \n",
        "4) What does damping do for Newton?\n"
      ],
      "metadata": {
        "id": "nkCs7rmLbaj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2 — Softmax Regression (Multiclass) with SGD (from scratch)\n",
        "Implement softmax regression on a 3-class toy dataset. We'll guide numerically stable parts.\n"
      ],
      "metadata": {
        "id": "G4Ot-1Sl_V3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xm, ym = make_toy_multiclass()\n",
        "Xmb = add_bias(Xm)\n",
        "plt.scatter(Xm[:,0], Xm[:,1], c=ym.ravel(), cmap=ListedColormap([\"#1f77b4\",\"#ff7f0e\",\"#2ca02c\"]), s=12)\n",
        "plt.title(\"Toy multiclass dataset\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cSv8WVf4_R78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Stable Softmax & Loss (Given)\n",
        "We provide stable softmax and cross-entropy loss (with L2 on weights, not bias row). You will implement the gradient.\n"
      ],
      "metadata": {
        "id": "rrs-kacibysp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GIVEN\n",
        "\n",
        "def softmax_logits(Z):\n",
        "    Zmax = np.max(Z, axis=1, keepdims=True)\n",
        "    expZ = np.exp(Z - Zmax)\n",
        "    return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "def softmax_loss(W, Xb, y_int, l2=1e-3):\n",
        "    Z = Xb @ W  # (n,C)\n",
        "    Zmax = np.max(Z, axis=1, keepdims=True)\n",
        "    logsumexp = Zmax + np.log(np.sum(np.exp(Z - Zmax), axis=1, keepdims=True))\n",
        "    n = Xb.shape[0]\n",
        "    rows = np.arange(n)\n",
        "    ll = -np.mean(Z[rows, y_int.ravel()].reshape(-1,1) - logsumexp)\n",
        "    ll += 0.5*l2*np.sum(W[:-1,:]**2)/n\n",
        "    return ll"
      ],
      "metadata": {
        "id": "s27gBG44b3bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Gradient & Training Loop (TODO)\n",
        "Implement the softmax gradient and a mini-batch SGD trainer.\n",
        "\n",
        "Hint: build one-hot targets, or compute class-wise residuals directly.\n"
      ],
      "metadata": {
        "id": "bG0oJZUOb6v5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: implement gradient and training\n",
        "\n",
        "def softmax_grad(W, Xb, y_int, l2=1e-3):\n",
        "    # TODO\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "def fit_softmax_sgd(Xb, y_int, l2=1e-3, lr=0.2, epochs=120, batch_size=32, verbose=False):\n",
        "    n, d = Xb.shape; C = int(y_int.max())+1\n",
        "    W = np.zeros((d, C))\n",
        "    hist = []\n",
        "    idx = np.arange(n)\n",
        "    for ep in range(epochs):\n",
        "        np.random.shuffle(idx)\n",
        "        for start in range(0, n, batch_size):\n",
        "            batch = idx[start:start+batch_size]\n",
        "            # TODO: compute gradient on batch and update W\n",
        "            pass\n",
        "        hist.append(softmax_loss(W, Xb, y_int, l2=l2))\n",
        "    return W, hist"
      ],
      "metadata": {
        "id": "58b32L4pAVTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Visualize Decision Regions (Given)"
      ],
      "metadata": {
        "id": "0aD9O_2hcNN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GIVEN\n",
        "\n",
        "def plot_multiclass_regions(W, X, y, title):\n",
        "    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
        "    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min,x_max,300), np.linspace(y_min,y_max,300))\n",
        "    Xg = np.c_[xx.ravel(), yy.ravel()]\n",
        "    Z = add_bias(Xg) @ W\n",
        "    pred = np.argmax(Z, axis=1).reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, pred, alpha=0.25, cmap=ListedColormap([\"#1f77b4\",\"#ff7f0e\",\"#2ca02c\"]))\n",
        "    plt.scatter(X[:,0], X[:,1], c=y.ravel(), cmap=ListedColormap([\"#1f77b4\",\"#ff7f0e\",\"#2ca02c\"]), s=12, edgecolor='k')\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "zs_LHB5kAXQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After implementing, run training and plot:"
      ],
      "metadata": {
        "id": "nIKxER7ocTiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage (uncomment after you implement):\n",
        "# W_smx, hist_smx = fit_softmax_sgd(Xmb, ym, l2=1e-3, lr=0.2, epochs=120, batch_size=32)\n",
        "# plt.plot(hist_smx); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Softmax SGD loss\"); plt.show()\n",
        "# plot_multiclass_regions(W_smx, Xm, ym, \"Softmax regression — SGD decision regions\")"
      ],
      "metadata": {
        "id": "2HuAVjencQz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Optional) Exercise 3 — Quick PyTorch Checkpoint\n",
        "Short, practical replication using PyTorch to connect your from-scratch math to a framework.\n",
        "Implement **either** binary logistic or multiclass softmax with the given skeleton.\n"
      ],
      "metadata": {
        "id": "1XvpO4SGcYwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to try, uncomment the cells below.\n",
        "# import torch\n",
        "# from torch import nn, optim\n",
        "# torch.manual_seed(42)\n",
        "\n",
        "# # Binary (logistic):\n",
        "# X_t = torch.tensor(X, dtype=torch.float32)\n",
        "# y_t = torch.tensor(y, dtype=torch.float32)\n",
        "# model = nn.Sequential(nn.Linear(2,1))\n",
        "# criterion = nn.BCEWithLogitsLoss()\n",
        "# opt = optim.SGD(model.parameters(), lr=0.2)\n",
        "# for ep in range(80):\n",
        "#     opt.zero_grad()\n",
        "#     logits = model(X_t)\n",
        "#     loss = criterion(logits, y_t)\n",
        "#     loss.backward(); opt.step()\n",
        "# print(\"Final loss (binary, torch):\", loss.item())\n",
        "\n",
        "# # Multiclass (softmax):\n",
        "# Xm_t = torch.tensor(Xm, dtype=torch.float32)\n",
        "# ym_t = torch.tensor(ym.squeeze(), dtype=torch.long)\n",
        "# model_m = nn.Sequential(nn.Linear(2, 3))\n",
        "# crit_m = nn.CrossEntropyLoss()\n",
        "# opt_m = optim.SGD(model_m.parameters(), lr=0.2)\n",
        "# for ep in range(120):\n",
        "#     opt_m.zero_grad()\n",
        "#     logits = model_m(Xm_t)\n",
        "#     loss = crit_m(logits, ym_t)\n",
        "#     loss.backward(); opt_m.step()\n",
        "# print(\"Final loss (multiclass, torch):\", loss.item())"
      ],
      "metadata": {
        "id": "RNbJUfR1cgkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "- Ensure all **TODOs** are implemented and cells run top-to-bottom\n",
        "- Include your brief **reflection answers**\n",
        "- Export and upload the executed `.ipynb` to Moodle\n"
      ],
      "metadata": {
        "id": "SCQB8WJ0cw4U"
      }
    }
  ]
}